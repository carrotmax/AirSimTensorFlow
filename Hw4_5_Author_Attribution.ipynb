{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Raw Cell Format",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "name": "Hw4_5_Author_Attribution.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carrotmax/AirSimTensorFlow/blob/master/Hw4_5_Author_Attribution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgdZwaELn_dC"
      },
      "source": [
        "# Introduction \n",
        "\n",
        "Author Attribution is a classic problem of NLP which is a part text classification problem. Authorship attribution is a well-studied problem which led to the field of [Stylometry](https://en.wikipedia.org/wiki/Stylometry).  Here, we given a set of documents from certain authors, we train a model to understand the author's style and use this to indentify the author of unknown documents. As with many other NLP problems, it has benefited greatly from the increase in available computer power, data and advanced machine learning techniques.  All of these make authorship attribution a natural candidate for the use of deep learning (DL).  In particular, we can benefit from DL's ability to automatically extract the relevant features for a specific problem.\n",
        "\n",
        "In this lab we will focus on the following:\n",
        "1.  Extract chracter level features from text of each author (to get author's style)\n",
        "2.  Using these features for building a classification model for authorship attribution\n",
        "3.  Applying the model for identifying the author of a set of unknown documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjxW8OYaq0Fc"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4XpnJFNrXMv",
        "outputId": "bac84da1-548e-44fc-90b4-4c281539d85c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-ecD_sOn_dE"
      },
      "source": [
        "As mentioned above, this problem can be solved in three steps. First is feature extraction. Here, since there is a limited amount of data, we are going to use character as our features instead of words or sentences. If we use words or sentences as our features, we are going to end up with small dataset which might be problematic to train our model.\n",
        "\n",
        "### Features\n",
        "\n",
        "1. A sequence of characters (length of sequence is a hyperparameter)\n",
        "2. An embedding layer for characters (dimensionality of the embedding is a hyperparameter)\n",
        "\n",
        "Embedding layer is a part of our model, but we can definitely consider it as feature extactor, since it encodes the features space into more meaningful semantic space. \n",
        "\n",
        "### Classifier\n",
        "\n",
        "1. Build a classifier using RNN layers and Dense layers. \n",
        "2. Choose an Optimizer, learning rate and train the model with extacted features\n",
        "\n",
        "### Predict\n",
        "\n",
        "1.  Break the entire document to sequences of the same length, as determined by the hyperparameter\n",
        "2.  Retrieve an author prediction for each one of these sequences\n",
        "3.  Determine which author has received more 'votes'.  We will then use this author as our prediction for the entire document.  (Note:  in order to have a clear majority, we need to ensure that the number of sequences is odd).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnD7OxVRn_dF"
      },
      "source": [
        "## Prepare the data\n",
        "\n",
        "We begin by setting up the data pre-processing pipeline.  For each one of the authors, we aggregate all the known papers into a single long text.  We assume that style does not change across the various papers, hence a single text is equivalent to multiple small ones yet it is much easier to deal with programmatically.\n",
        "\n",
        "For each paper of each author we perform the following steps:\n",
        "1. Convert all text into lower-case (ignoring the fact that capitalization may be a stylistic property)\n",
        "2. Converting all newlines and multiple whitespaces into single whitespaces\n",
        "3. Remove any mention of the authors' names, otherwise we risk data leakage (authors names are hamilton and madison)\n",
        "\n",
        "Do the above steps in a function as it is needed for predicting the unknown papers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Sa0qNbrrn_dG",
        "outputId": "bdac6e53-bc96-4024-ba1a-e58e16653728",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Classes for A/B/Unknown\n",
        "A = 0\n",
        "B = 1\n",
        "UNKNOWN = -1\n",
        "\n",
        "\n",
        "def preprocess_text(file_path):\n",
        "\n",
        "   with open(file_path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        text = ' '.join(lines[1:]).replace(\"\\n\", ' ').replace('  ',' ').lower().replace('hamilton','').replace('madison', '')\n",
        "        text = ' '.join(text.split())\n",
        "        return text\n",
        "\n",
        "\n",
        "# Concatenate all the papers known to be written by A/B into a single long text\n",
        "all_authorA, all_authorB = '',''\n",
        "for x in os.listdir('drive/My Drive/Colab Notebooks/papers/A/'):\n",
        "    all_authorA += preprocess_text('drive/My Drive/Colab Notebooks/papers/A/' + x)\n",
        "\n",
        "for x in os.listdir('drive/My Drive/Colab Notebooks/papers/B/'):\n",
        "    all_authorB += preprocess_text('drive/My Drive/Colab Notebooks/papers/B/' + x)\n",
        "    \n",
        "# Print lengths of the large texts\n",
        "print(\"AuthorA text length: {}\".format(len(all_authorA)))\n",
        "print(\"AuthorB text length: {}\".format(len(all_authorB)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AuthorA text length: 216394\n",
            "AuthorB text length: 230867\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "QeIJiD45y4uq",
        "outputId": "d8e3c0de-9cc4-43cc-9b73-4b1b597da53f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Classes for A/B/Unknown\n",
        "A = 0\n",
        "B = 1\n",
        "UNKNOWN = -1\n",
        "\n",
        "\n",
        "def preprocess_text(file_path):\n",
        "\n",
        "   with open(file_path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        text = ' '.join(lines[1:]).replace(\"\\n\", ' ').replace('  ',' ').lower().replace('hamilton','').replace('madison', '')\n",
        "        text = ' '.join(text.split())\n",
        "        return text\n",
        "\n",
        "\n",
        "# Concatenate all the papers known to be written by A/B into a single long text\n",
        "all_authorA, all_authorB = '',''\n",
        "for x in os.listdir('drive/My Drive/Colab Notebooks/papers/A/'):\n",
        "    all_authorA += preprocess_text('drive/My Drive/Colab Notebooks/papers/A/' + x)\n",
        "\n",
        "for x in os.listdir('drive/My Drive/Colab Notebooks/papers/B/'):\n",
        "    all_authorB += preprocess_text('drive/My Drive/Colab Notebooks/papers/B/' + x)\n",
        "    \n",
        "# Print lengths of the large texts\n",
        "print(\"AuthorA text length: {}\".format(len(all_authorA)))\n",
        "print(\"AuthorB text length: {}\".format(len(all_authorB)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AuthorA text length: 216394\n",
            "AuthorB text length: 230867\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "S7YYYXoYy63z",
        "outputId": "c6e156ee-3e28-460f-a468-bb317a39512b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Classes for A/B/Unknown\n",
        "A = 0\n",
        "B = 1\n",
        "UNKNOWN = -1\n",
        "\n",
        "\n",
        "def preprocess_text(file_path):\n",
        "\n",
        "   with open(file_path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        text = ' '.join(lines[1:]).replace(\"\\n\", ' ').replace('  ',' ').lower().replace('hamilton','').replace('madison', '')\n",
        "        text = ' '.join(text.split())\n",
        "        return text\n",
        "\n",
        "\n",
        "# Concatenate all the papers known to be written by A/B into a single long text\n",
        "all_authorA, all_authorB = '',''\n",
        "for x in os.listdir('drive/My Drive/Colab Notebooks/papers/A/'):\n",
        "    all_authorA += preprocess_text('drive/My Drive/Colab Notebooks/papers/A/' + x)\n",
        "\n",
        "for x in os.listdir('drive/My Drive/Colab Notebooks/papers/B/'):\n",
        "    all_authorB += preprocess_text('drive/My Drive/Colab Notebooks/papers/B/' + x)\n",
        "    \n",
        "# Print lengths of the large texts\n",
        "print(\"AuthorA text length: {}\".format(len(all_authorA)))\n",
        "print(\"AuthorB text length: {}\".format(len(all_authorB)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AuthorA text length: 216394\n",
            "AuthorB text length: 230867\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhvODEuXn_dN"
      },
      "source": [
        "The next step is to break the long text for each author into many small sequences.  As described above, we empirically choose a length for the sequence and use it throughout the model's lifecycle.  We get our full dataset by labeling each sequence with its author.\n",
        "\n",
        "To break the long texts into smaller sequences we use the *Tokenizer* class from the Keras framework.  In particular, note that we set it up to tokenize according to *characters* and not words.\n",
        "\n",
        "1. Choose SEQ_LEN hyper parameter, this might have to be changed if the model doesn't fit well to training data. \n",
        "2. Write a function make_subsequences to turn each document into sequences of length SEQ_LEN and give it a correct label.\n",
        "3. Use keras Tokenizer with char_level=True\n",
        "4. fit the tokenizer on all the texts\n",
        "5. Use this tokenizer to convert all texts into sequences using texts_to_sequences()\n",
        "6. Use make_subsequences() to turn these sequences into appropriate shape and length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAGnDMDfn_dN",
        "outputId": "a8774ffd-ad4f-4ed4-c51d-17beee8a1bc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "\n",
        "# Hyperparameter - sequence length to use for the model\n",
        "SEQ_LEN = 30\n",
        "\n",
        "\n",
        "def make_subsequences(long_sequence, label, sequence_length=SEQ_LEN):\n",
        "\n",
        "    len_sequences = len(long_sequence)\n",
        "    X = np.zeros(((len_sequences - sequence_length)+1, sequence_length))\n",
        "    y = np.zeros((X.shape[0], 1))\n",
        "    for i in range(X.shape[0]):\n",
        "        X[i] = long_sequence[i:i+sequence_length]\n",
        "        y[i] = label\n",
        "    return X,y\n",
        "        \n",
        "# We use the Tokenizer class from Keras to convert the long texts into a sequence of characters (not words)\n",
        "\n",
        "tokenizer = Tokenizer(char_level=True)\n",
        "\n",
        "# Make sure to fit all characters in texts from both authors\n",
        "tokenizer.fit_on_texts(all_authorA + all_authorB)\n",
        "\n",
        "authorA_long_sequence = tokenizer.texts_to_sequences([all_authorA])[0]\n",
        "authorB_long_sequence = tokenizer.texts_to_sequences([all_authorB])[0]\n",
        "\n",
        "# Convert the long sequences into sequence and label pairs\n",
        "X_authorA, y_authorA = make_subsequences(authorA_long_sequence, A)\n",
        "X_authorB, y_authorB = make_subsequences(authorB_long_sequence, B)\n",
        "\n",
        "# Print sizes of available data\n",
        "print(\"Number of characters: {}\".format(len(tokenizer.word_index)))\n",
        "print('author A sequences: {}'.format(X_authorA.shape))\n",
        "print('author B sequences: {}'.format(X_authorB.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of characters: 52\n",
            "author A sequences: (216365, 30)\n",
            "author B sequences: (230838, 30)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQxx1fDEn_dR"
      },
      "source": [
        "Compare the number of raw characters to the number of labeled sequences for each author.  Deep Learning requires many examples of each input.  The following code calculates the number of total and unique words in the texts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdbkf5WGn_dR",
        "outputId": "bb20ef7d-c45f-49d3-cf42-4d4e5398cac7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Calculate the number of unique words in the text\n",
        "\n",
        "word_tokenizer = Tokenizer()\n",
        "word_tokenizer.fit_on_texts([all_authorA, all_authorB])\n",
        "\n",
        "print(\"Total word count: \", len((all_authorA + ' ' + all_authorB).split(' ')))\n",
        "print(\"Total number of unique words: \", len(word_tokenizer.word_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total word count:  74349\n",
            "Total number of unique words:  6318\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kznajw3An_dV"
      },
      "source": [
        "We now proceed to create our train, validation sets.  \n",
        "\n",
        "1. Stack x data together and y data together\n",
        "2. use train_test_split to split the dataset into 80% training and 20% validation\n",
        "3. Reshape the data to make sure that they are sequences of correct length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnwZ_iezn_dV",
        "outputId": "f30f9581-6682-40e5-b2df-73122a58875c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "# Take equal amounts of sequences from both authors\n",
        "X = np.vstack((X_authorA, X_authorB))\n",
        "y = np.vstack((y_authorA, y_authorB))\n",
        "\n",
        "# Break data into train and test sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X,y, train_size=0.8)\n",
        "\n",
        "# Data is to be fed into RNN - ensure that the actual data is of size [batch size, sequence length]\n",
        "X_train = X_train.reshape(-1, SEQ_LEN)\n",
        "X_val =  X_val.reshape(-1, SEQ_LEN) \n",
        "\n",
        "# Print the shapes of the train, validation and test sets\n",
        "print(\"X_train shape: {}\".format(X_train.shape))\n",
        "print(\"y_train shape: {}\".format(y_train.shape))\n",
        "\n",
        "print(\"X_validate shape: {}\".format(X_val.shape))\n",
        "print(\"y_validate shape: {}\".format(y_val.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape: (357762, 30)\n",
            "y_train shape: (357762, 1)\n",
            "X_validate shape: (89441, 30)\n",
            "y_validate shape: (89441, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Dc5Kx8sn_db"
      },
      "source": [
        "Finally, we construct the model graph and perform the training procedure.\n",
        "\n",
        "1. Create a model using RNN and Dense layers\n",
        "2. Since its a binary classification problem, the output layer should be Dense with sigmoid activation\n",
        "3. Compile the model with optimizer, appropriate loss function and metrics\n",
        "4. Print the summary of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ExF_CTEn_dd",
        "outputId": "1ae56f97-fc27-4012-cdd5-a51ec5d0a8cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "from keras.layers import SimpleRNN, Embedding, Dense\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import SGD, Adadelta, Adam\n",
        "Embedding_size = 100\n",
        "RNN_size = 256\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(tokenizer.word_index)+1, Embedding_size, input_length=30))\n",
        "model.add(SimpleRNN(RNN_size, return_sequences=False))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics = ['accuracy'])\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 30, 100)           5300      \n",
            "_________________________________________________________________\n",
            "simple_rnn (SimpleRNN)       (None, 256)               91392     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 96,949\n",
            "Trainable params: 96,949\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NB62Ukp6n_dh"
      },
      "source": [
        "1. Decide upon the batch size, epochs and train the model using training data and validate with vailadation data\n",
        "2. Based on the results, go back to model above, change it if needed ( use more layers, use regularization, dropout, etc., use different optimizer, or a different learning rate, etc.)\n",
        "3. Change Batch size, epochs if needed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9at1g6ln_di",
        "outputId": "27022ced-01f9-4204-b248-ae3d2a31e181",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        }
      },
      "source": [
        "Batch_size = 4096\n",
        "Epochs = 20\n",
        "model.fit(X_train, y_train, batch_size=Batch_size, epochs=Epochs, validation_data=(X_val, y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "88/88 [==============================] - 150s 2s/step - loss: 0.6891 - accuracy: 0.5349 - val_loss: 0.6844 - val_accuracy: 0.5526\n",
            "Epoch 2/20\n",
            "88/88 [==============================] - 149s 2s/step - loss: 0.6830 - accuracy: 0.5561 - val_loss: 0.6829 - val_accuracy: 0.5570\n",
            "Epoch 3/20\n",
            "88/88 [==============================] - 150s 2s/step - loss: 0.6828 - accuracy: 0.5576 - val_loss: 0.6836 - val_accuracy: 0.5525\n",
            "Epoch 4/20\n",
            "88/88 [==============================] - 151s 2s/step - loss: 0.6809 - accuracy: 0.5586 - val_loss: 0.6797 - val_accuracy: 0.5611\n",
            "Epoch 5/20\n",
            "88/88 [==============================] - 148s 2s/step - loss: 0.6775 - accuracy: 0.5662 - val_loss: 0.6725 - val_accuracy: 0.5764\n",
            "Epoch 6/20\n",
            "88/88 [==============================] - 150s 2s/step - loss: 0.6645 - accuracy: 0.5916 - val_loss: 0.6573 - val_accuracy: 0.6029\n",
            "Epoch 7/20\n",
            "88/88 [==============================] - 148s 2s/step - loss: 0.6412 - accuracy: 0.6275 - val_loss: 0.6392 - val_accuracy: 0.6379\n",
            "Epoch 8/20\n",
            "88/88 [==============================] - 148s 2s/step - loss: 0.6062 - accuracy: 0.6658 - val_loss: 0.5953 - val_accuracy: 0.6803\n",
            "Epoch 9/20\n",
            "88/88 [==============================] - 148s 2s/step - loss: 0.5615 - accuracy: 0.7086 - val_loss: 0.5391 - val_accuracy: 0.7248\n",
            "Epoch 10/20\n",
            "88/88 [==============================] - 145s 2s/step - loss: 0.4969 - accuracy: 0.7573 - val_loss: 0.4712 - val_accuracy: 0.7742\n",
            "Epoch 11/20\n",
            "88/88 [==============================] - 144s 2s/step - loss: 0.4202 - accuracy: 0.8055 - val_loss: 0.4061 - val_accuracy: 0.8120\n",
            "Epoch 12/20\n",
            "88/88 [==============================] - 144s 2s/step - loss: 0.3514 - accuracy: 0.8463 - val_loss: 0.3351 - val_accuracy: 0.8521\n",
            "Epoch 13/20\n",
            "88/88 [==============================] - 145s 2s/step - loss: 0.2864 - accuracy: 0.8772 - val_loss: 0.2923 - val_accuracy: 0.8748\n",
            "Epoch 14/20\n",
            "88/88 [==============================] - 145s 2s/step - loss: 0.2357 - accuracy: 0.9027 - val_loss: 0.2505 - val_accuracy: 0.8961\n",
            "Epoch 15/20\n",
            "88/88 [==============================] - 145s 2s/step - loss: 0.1969 - accuracy: 0.9205 - val_loss: 0.2146 - val_accuracy: 0.9117\n",
            "Epoch 16/20\n",
            "88/88 [==============================] - 145s 2s/step - loss: 0.1675 - accuracy: 0.9334 - val_loss: 0.1938 - val_accuracy: 0.9206\n",
            "Epoch 17/20\n",
            "88/88 [==============================] - 144s 2s/step - loss: 0.1466 - accuracy: 0.9426 - val_loss: 0.1794 - val_accuracy: 0.9276\n",
            "Epoch 18/20\n",
            "88/88 [==============================] - 144s 2s/step - loss: 0.1241 - accuracy: 0.9523 - val_loss: 0.1633 - val_accuracy: 0.9343\n",
            "Epoch 19/20\n",
            "88/88 [==============================] - 145s 2s/step - loss: 0.1097 - accuracy: 0.9582 - val_loss: 0.1640 - val_accuracy: 0.9363\n",
            "Epoch 20/20\n",
            "88/88 [==============================] - 145s 2s/step - loss: 0.0982 - accuracy: 0.9634 - val_loss: 0.1404 - val_accuracy: 0.9459\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe39147ff98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yG7rq6Mln_dp"
      },
      "source": [
        "### Applying the Model to the Unknown Papers\n",
        "\n",
        "Do this all the papers in the Unknown folder\n",
        "1. preprocess them same way as training set (lower case, removing white lines, etc.)\n",
        "2. use tokenizer and make_subsequences function above to turn them into sequences of required size\n",
        "3. Use the model to predict on these sequences.\n",
        "4. Count the number of sequences assigned to author A and the ones assigned to author B\n",
        "5. Based on the count, pick the author with highest votes/count"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oko4mA7wn_dq",
        "outputId": "3e55caf7-77e9-4c84-a899-6fc1cde378b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "for x in os.listdir('drive/My Drive/Colab Notebooks/papers/Unknown/'):\n",
        "    unknown = preprocess_text('drive/My Drive/Colab Notebooks/papers/Unknown/' + x)\n",
        "    unknown_long_sequences = tokenizer.texts_to_sequences([unknown])[0]\n",
        "    X_sequences, _ = make_subsequences(unknown_long_sequences, UNKNOWN)\n",
        "    X_sequences = X_sequences.reshape((-1,SEQ_LEN))\n",
        "    \n",
        "    votes_for_authorA = 0\n",
        "    votes_for_authorB = 0\n",
        "    \n",
        "    y = model.predict(X_sequences)\n",
        "    y = y>0.5\n",
        "    votes_for_authorA = np.sum(y==0)\n",
        "    votes_for_authorB = np.sum(y==1)\n",
        "    \n",
        "    \n",
        "    print(\"Paper {} is predicted to have been written by {}, {} to {}\".format(\n",
        "                x.replace('paper_','').replace('.txt',''), \n",
        "                (\"Author A\" if votes_for_authorA > votes_for_authorB else \"Author B\"),\n",
        "                max(votes_for_authorA, votes_for_authorB), min(votes_for_authorA, votes_for_authorB)))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Paper 1 is predicted to have been written by Author B, 12898 to 7876\n",
            "Paper 3 is predicted to have been written by Author B, 7225 to 6159\n",
            "Paper 4 is predicted to have been written by Author A, 4937 to 4836\n",
            "Paper 5 is predicted to have been written by Author A, 6669 to 5085\n",
            "Paper 2 is predicted to have been written by Author B, 12252 to 7394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SO9JuiY6n_dt"
      },
      "source": [
        "# Summary\n",
        "\n",
        "In this lab, we discussed the problem of authorship attribution.  Finally, we looked at the model internals to get an intuition for how the it encodes stylometric properties.\n",
        "\n",
        "The first two papers are written by author B, and next three papers are written by author A.\n",
        "\n",
        "The model was able capture the style of each author based on the character sequences given to it. This is a hyper parameter which needs to be tuned. So, play with this parameter as a part of feature extarcation stage.\n",
        "\n",
        "Finally, you are able to train a model to solve author attribution.\n",
        "\n",
        "Good luck for your next lessons! Do try this assigment with the layers you learn next and see if there is any improvement in the model."
      ]
    }
  ]
}